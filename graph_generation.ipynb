{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RelaTree - Social Graph Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been created to create a social graph based on the given data about user and their groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    '--jars lib/graphframes-release-0-5-0-assembly-0.5.0-spark2.1.jar pyspark-shell')\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "import graphframes as GF\n",
    "import operator\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, size, lit, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function: Create spark context\n",
    "Parameters: app_name, executor_memory, no_of_executors\n",
    "Returns: Spark SQLContext\n",
    "'''\n",
    "def createContext(app_name=\"RelaTree\", executor_memory=\"2g\", no_of_executors=\"4\"):\n",
    "    sparkConf = (SparkConf().setMaster(\"local\").setAppName(app_name).set(\"spark.executor.memory\", executor_memory).set(\"spark.executor.instances\", no_of_executors))\n",
    "    sparkContext = SparkContext(conf=sparkConf)\n",
    "    sql_context = SQLContext(sparkContext)\n",
    "    return sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Create Spark dataframe from csv file\n",
    "Parameters: file_path,sql_context\n",
    "Returns: Pyspark Dataframe\n",
    "'''\n",
    "def loadData(file_path,sql_context):\n",
    "    df = sql_context.read.format('com.databricks.spark.csv').options(header='true').load(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Provide stats for a Pyspark Dataframe\n",
    "Parameters: Pyspark Dataframe\n",
    "Returns: N/A\n",
    "'''\n",
    "def getStats(df):\n",
    "    print(\"\\nRow count: %d\\n\\nColumn Count: %d\\n\\nColumn headers: %s\\n\\nSample Data:\\n\" %(df.count(),len(df.columns),df.columns))\n",
    "    df.show(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Create vertices dataframe\n",
    "Parameters: Pyspark Dataframe\n",
    "Returns: Pyspark dataframe depciting vertices\n",
    "'''\n",
    "def createUserGraphVertices(df):\n",
    "    print(\"Creating User Vertices DataFrame..\")\n",
    "    # Select user_Id [vertices] column from Spark dataframe\n",
    "    df_users = df.select(['user_id'])\n",
    "    df_users = df_users.selectExpr(\"user_id as id\") \n",
    "    # Remove duplicate user_id entries and create vertices dataframe\n",
    "    vertices = df_users.drop_duplicates()\n",
    "    print(\"User Vertices DataFrame creation complete.\")\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Create edges dataframe\n",
    "Parameters: Pyspark Dataframe\n",
    "Returns: Pyspark dataframe depciting edges\n",
    "'''\n",
    "def createUserGraphEdges(df):\n",
    "    # Create the edges dataframe\n",
    "    print(\"Creating User Edges DataFrame..\")\n",
    "    edges = df.select(col('user_id').alias('src'),col('group_id')).join(df.select(col('user_id').alias('dst'),col('group_id')), on=['group_id'], how='outer')\n",
    "    # Remove duplicate entries\n",
    "    edges = edges.drop_duplicates()\n",
    "    print(\"User Edges DataFrame creation complete.\")\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Create graph\n",
    "Parameters: Pyspark Dataframe - vertices, edges\n",
    "Returns: GraphFrame\n",
    "'''\n",
    "def createGraph(vertices, edges):\n",
    "    print(\"Creating graph..\")\n",
    "    # Generate the graph\n",
    "    graph = GF.GraphFrame(vertices, edges)\n",
    "    print(\"Graph creation complete.\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Save graph to file\n",
    "Parameters: GraphFrame\n",
    "Returns: N/A\n",
    "'''\n",
    "def saveGraph(graph, name):\n",
    "    # Save the graph to a file\n",
    "    print(\"Saving graph to file..\")\n",
    "    graph.vertices.write.parquet('store/'+name+'Vertices.parquet')\n",
    "    graph.edges.write.parquet('store/'+name+'Edges.parquet')\n",
    "    print(\"Graph has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Load graph from file\n",
    "Parameters: N/A\n",
    "Returns: GraphFrame\n",
    "'''\n",
    "def loadGraph(context, name):\n",
    "    # Load the graph from file\n",
    "    print(\"\\nLoading graph data..\")\n",
    "    vertices = context.read.parquet('store/'+name+'Vertices.parquet')\n",
    "    edges = context.read.parquet('store/'+name+'Edges.parquet')\n",
    "    print(\"\\nGenerating graph..\")\n",
    "    graph = GF.GraphFrame(vertices, edges)\n",
    "    print(\"\\nGraph load complete.\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Obtain the first connects of a given vertex\n",
    "Parameters: GraphFrame, vertex label\n",
    "Returns: GraphFrame of connected vertices and their edges\n",
    "'''\n",
    "def firstConnects(graph, vertex):\n",
    "    first_connect_motifs = graph.find(\"(v1)-[e]->(v2)\").filter(\"v1.id == '\"+vertex+\"'\")\n",
    "    return first_connect_motifs.select(\"v2.id\",\"e.group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Delete dataframe to free memory\n",
    "Parameters: List of DataFrames\n",
    "Returns: N/A\n",
    "'''\n",
    "def cleanUp(df_list):\n",
    "    for df in df_list:\n",
    "        del df\n",
    "    print(\"\\nDataFrame clean up complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Get users connected by the given group\n",
    "Parameters: Graphframe, group_id\n",
    "Returns: Set of users\n",
    "'''\n",
    "def getUsersOfAGroup(graph, group):\n",
    "    print(\"\\nGetting Users..\")\n",
    "    edges = graph.edges.filter(\"group_id = '\"+group+\"'\").collect()\n",
    "    users = set()\n",
    "    for row in edges:\n",
    "        users.add(row.src)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Add first connected users to the given user set\n",
    "Parameters: Graphframe, users set\n",
    "Returns: Set of users\n",
    "'''\n",
    "def addFirstConnects(graph, users):\n",
    "    print(\"\\nAdding first connects..\")\n",
    "    newUsers = set()\n",
    "    for user in users:\n",
    "        v = firstConnects(graph, user)\n",
    "        v = v.collect()\n",
    "        for row in v:\n",
    "            newUsers.add(row.id)\n",
    "    users = users.union(newUsers)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Get all the groups associated with the given users\n",
    "Parameters: Graphframe, users set\n",
    "Returns: Dictionary of Groups with their counts frequency\n",
    "'''\n",
    "def getGroupsAssociatedToUsers(graph, users):\n",
    "    print(\"\\nGetting groups..\")\n",
    "    groups = dict()\n",
    "    for user in users:\n",
    "        g = graph.edges.filter(\"src = '\"+user+\"'\").collect()\n",
    "        for row in g:\n",
    "            key = row.group_id\n",
    "            if key in groups:\n",
    "                groups[key] += 1\n",
    "            else:\n",
    "                groups[key] = 1\n",
    "    total = 0\n",
    "    for key in groups.keys():\n",
    "        total += groups[key]\n",
    "    for key in groups.keys():\n",
    "        groups[key] = groups[key] / total\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Get all the channels associated with the given groups\n",
    "Parameters: Pyspark Dataframe, groups dict\n",
    "Returns: Dictionary of Channels with their counts frequency weighted based on groups\n",
    "'''\n",
    "def getChannelsAssociatedToGroups(df, groups):\n",
    "    print(\"\\nGetting channels..\")\n",
    "    channels = dict()\n",
    "    for group in groups.keys():\n",
    "        c = df.filter(\"group_id = '\"+group+\"'\").collect()\n",
    "        for row in c:\n",
    "            key = row.channel_id\n",
    "            if key in channels:\n",
    "                channels[key] += groups[group]\n",
    "            else:\n",
    "                channels[key] = groups[group]\n",
    "    total = 0\n",
    "    for key in channels.keys():\n",
    "        total += channels[key]\n",
    "    for key in channels.keys():\n",
    "        channels[key] = channels[key] / total\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Get all the channels associated with the given group along with their respective importance factor\n",
    "Parameters: Pyspark Dataframe, Graphframe, group_id\n",
    "Returns: Dictionary of Channels with their counts frequency weighted based on groups\n",
    "'''\n",
    "def getChannelsForGroup(df, graph, group):\n",
    "    users = getUsersOfAGroup(graph, group)\n",
    "    users = addFirstConnects(graph, users)\n",
    "    groups = getGroupsAssociatedToUsers(graph, users)\n",
    "    channels = getChannelsAssociatedToGroups(df, groups)\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Create channel vertices dataframe\n",
    "Parameters: Pyspark Dataframe\n",
    "Returns: Pyspark dataframe depciting channel vertices\n",
    "'''\n",
    "def createChannelGraphVetrices(df):\n",
    "    print(\"Creating Channel Vertices DataFrame..\")\n",
    "    # Select channel_id [vertices] column from Spark dataframe\n",
    "    df_channels = df.select(['channel_id'])\n",
    "    df_channels = df_channels.selectExpr('channel_id as id') \n",
    "    # Remove duplicate user_id entries and create vertices dataframe\n",
    "    vertices = df_channels.drop_duplicates()\n",
    "    print(\"Channel Vertices DataFrame creation complete.\")\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Create channel edges dataframe\n",
    "Parameters: Pyspark Dataframe\n",
    "Returns: Pyspark dataframe depciting channel edges\n",
    "'''\n",
    "def createChannelGraphEdges(df):\n",
    "    # Create the edges dataframe\n",
    "    print(\"Creating Channel Edges DataFrame..\")\n",
    "    channel_group = df.select(col('channel_id').alias('src'),col('group_id')).join(df.select(col('channel_id').alias('dst'),col('group_id')), on=['group_id'], how='outer')\n",
    "    channel_group_filtered = channel_group.filter(channel_group.src != channel_group.dst)\n",
    "    channel_group_count = channel_group_filtered.groupby(['src','dst']).count()\n",
    "    channel_edges = channel_vertices.select(col('id').alias('src')).crossJoin(channel_vertices.select(col('id').alias('dst')))\n",
    "    channel_edges = channel_edges.filter(channel_edges.src != channel_edges.dst)\n",
    "    channel_edges_weighted = channel_edges.withColumn(\"init_weight\", lit(1))\n",
    "    channel_edge_weighted_joined = channel_edges_weighted.join(channel_group_count, on=['src','dst'], how='left_outer')\n",
    "    channel_edge_weighted_joined = channel_edge_weighted_joined.na.fill(0)\n",
    "    channel_edge_weighted_final = channel_edge_weighted_joined.withColumn('weight', channel_edge_weighted_joined['init_weight']+channel_edge_weighted_joined['count'])\n",
    "    channel_edge_weighted_final = channel_edge_weighted_final.select('src','dst','weight')\n",
    "    print(\"Channel Edges DataFrame creation complete.\")\n",
    "    return (channel_edge_weighted_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Get recommendations for the given group\n",
    "Parameters: User Graphframe, Channel Graphframe, Pyspark Dataframe, group_id, k (number of recommendations)\n",
    "Returns: List of tuples with channel_id and recommendation score\n",
    "'''\n",
    "def graph_recommender(user_graph, channel_graph, df, group, k):\n",
    "    print(\"\\nFinding Recommendations..\")\n",
    "    channels = getChannelsForGroup(df, user_graph, group)\n",
    "    recommendations = {}\n",
    "    for channel in channels.keys():\n",
    "        channel_motif = channel_graph.find(\"(v1)-[e]->(v2)\").filter(\"v1.id == '\"+channel+\"'\")\n",
    "        direct_neighbors = channel_motif.select(\"v2.id\",\"e.weight\").collect()\n",
    "        for row in direct_neighbors:\n",
    "            if row.id not in channels:\n",
    "                recommendations[row.id]=recommendations.get(row.id,0)+(row.weight*channels[channel])\n",
    "    print(\"\\nRecommendations found.\")\n",
    "    return (sorted(recommendations.items(), key=operator.itemgetter(1),reverse=True))[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading graph data..\n",
      "\n",
      "Generating graph..\n",
      "\n",
      "Graph load complete.\n",
      "\n",
      "Loading graph data..\n",
      "\n",
      "Generating graph..\n",
      "\n",
      "Graph load complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create Spark context\n",
    "    sql_context = createContext(app_name=\"RelaTree\", executor_memory=\"12g\", no_of_executors=\"8\")\n",
    "    \n",
    "    # Load group to member data into a Pyspark Dataframe\n",
    "#     df_group_members = loadData('data/group_members.csv',sql_context)\n",
    "    \n",
    "    # Load group to channel data into a Pyspark Dataframe\n",
    "    df_group_channels = loadData('data/group_channel.csv',sql_context)\n",
    "    \n",
    "    # Get stats on group members and group channels dataframes\n",
    "#     getStats(df_group_members)\n",
    "#     getStats(df_group_channels)\n",
    "    \n",
    "    # Create vertices dataframe\n",
    "#     user_vertices = createUserGraphVertices(df_group_members)\n",
    "#     channel_vertices = createChannelGraphVetrices(df_group_channels)\n",
    "\n",
    "    # Create edges dataframe\n",
    "#     user_edges = createEdges(df_group_members)\n",
    "#     channel_edges = createChannelGraphEdges(df_group_channels)\n",
    "    \n",
    "    # Get stats on vertices and edges dataframes\n",
    "#     getStats(user_vertices)\n",
    "#     getStats(user_edges)\n",
    "#     getStats(channel_vertices)\n",
    "#     getStats(channel_edges)\n",
    "    \n",
    "    # Create Graph\n",
    "#     duta_user_graph = createGraph(vertices,edges)\n",
    "#     duta_channel_graph = createGraph(channel_vertices,channel_edges)\n",
    "    \n",
    "    # Clean up memory\n",
    "#     cleanUp([user_vertices, user_edges, channel_vertices, channel_edges, df_group_members])\n",
    "    \n",
    "    # Save Graph to file \n",
    "#     saveGraph(duta_user_graph, 'userGraph)\n",
    "#     saveGraph(duta_channel_graph, 'channelGraph')    \n",
    "    \n",
    "    # Load graph from file\n",
    "    duta_user_graph = loadGraph(sql_context, 'userGraph')\n",
    "    duta_channel_graph = loadGraph(sql_context, 'channelGraph')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding Recommendations..\n",
      "\n",
      "Getting Users..\n",
      "\n",
      "Adding first connects..\n",
      "\n",
      "Getting groups..\n"
     ]
    }
   ],
   "source": [
    "result = graph_recommender(duta_user_graph, duta_channel_graph, df_group_channels,'0000105d4a97a48cbd1a0968695c8f97f54ffe23' , 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting Users..\n",
      "\n",
      "Adding first connects..\n",
      "\n",
      "Getting groups..\n",
      "\n",
      "Getting channels..\n"
     ]
    }
   ],
   "source": [
    "channels = getChannelsForGroup(df_group_channels, duta_user_graph, 'c141ef4d309b251fff0d46dc9e70a8f37f8d83dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'promo': 0.5, 'cricket': 0.5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
